## Contact Details

Michael Ghen mike@mikeghen.com

## Type Name

`ChatOpenAI`

## Description

This interface uses the OpenAI API to generate responses to prompts. It differs from `ChatGPTResponse` as this allows tuning:

* System Prompt
* Model
* Temperature

It requires parameters `systemPrompt` and `userPrompt`, `model`, and `temperature` for query execution. The response will result from executing a newly initialized [`langchain_openai.ChatOpenAI`](https://python.langchain.com/docs/integrations/chat/openai/) model with these parameters.

## Query Parameters

The parameters for a `ChatOpenAI` query are defined as follows:

1. **systemPrompt**
   - **Description:** The prompt used as the system prompt for a `langchain_openai.ChatOpenAI` model execution.
   - **Type:** `string`
   
2. **userPrompt**
   - **Description:** The user's input prompt to the model.
   - **Type:** `string`
   
3. **model**
   - **Description:** The OpenAI model to use for generating responses.
   - **Type:** `string`
   
4. **temperature**
   - **Description:** The temperature setting for the OpenAI model, influencing the randomness of the response. This should be converted to a scale of 0 to 100 for on-chain representation 0 to 1.
   - **Type:** `uint8`

## Response Type

Responses are returned as strings.

1. **output**
   - **Description:** The response generated by the OpenAI model based on the provided prompts and parameters. 
   - **Type:** `string`

## Query Data

Query data forms the unique identifier for your query and is included in emitted contract events for programmability.

To form the query data for a `ChatOpenAI` query, encode the parameters' values in UTF-8, in the specified order, then encode these bytes with the query type string.

For instance, in Solidity:

```solidity
string systemPrompt = "You're a developer";
string userPrompt = "What is Tellor?";
string model = "gpt-3";
uint8 temperature = 10; // Represented as 0.1 on a scale of 0 to 100 for blockchain
bytes queryData = abi.encode("ChatOpenAI", abi.encode(systemPrompt, userPrompt, model, temperature));
```

## Query ID

Generate a `bytes32` value for the `keccak` hash of the query data to obtain your query's unique identifier. For example:

```solidity
bytes32 queryId = keccak256(queryData);
```

## JSON Representation

To construct query objects across different programming languages, the JSON representation of your query includes the type name, ordered parameters with their types, and the expected response type.

Example for a `ChatOpenAI` query:

```json
{
    "type": "ChatOpenAI",
    "parameters": [
        {"name": "systemPrompt", "type": "string"},
        {"name": "userPrompt", "type": "string"},
        {"name": "model", "type": "string"},
        {"name": "temperature", "type": "uint8"}
    ],
    "response": {
        "type": "string"
    }
}
```

## Example

### Request

To request a query that sets the system prompt to "You're a developer," the user prompt as "Tell me about yourself in 500 words or less," the model as "gpt-3," and the temperature as 0.1:

```solidity
bytes memory queryData = abi.encode(
    "ChatOpenAI",
    abi.encode(
        "You're a developer", 
        "What is Tellor?", 
        "gpt-3", 
        10
    )
);
bytes32 queryId = keccak256(queryData);
```

queryData: `0x00000000000000000000000000000000000000000000000000000000000000400000000000000000000000000000000000000000000000000000000000000080000000000000000000000000000000000000000000000000000000000000000a436861744f70656e4149000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000140000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000c00000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000a0000000000000000000000000000000000000000000000000000000000000012596f75277265206120646576656c6f7065720000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000f576861742069732054656c6c6f723f000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000056770742d33000000000000000000000000000000000000000000000000000000`


queryId: `0xc63f92f4a9e91366fb68a93a905d09c8a09ab80230dba7be9bf24420dfb4c9b1`


### Response

Response type: string

Example response: `bytes response = 0000000000000000000000000000000000000000000000000000000000000020000000000000000000000000000000000000000000000000000000000000020454656c6c6f72206973206120646563656e7472616c697a6564204f7261636c6520706c6174666f726d207468617420616c6c6f777320757365727320746f207365637572656c7920616e64207472616e73706172656e746c79207265717565737420646174612066726f6d2065787465726e616c20736f75726365732e2054686520706c6174666f726d20757365732061206e6574776f726b206f66206d61737465726e6f64657320746f2067617468657220616e642076616c696461746520646174612c207768696368206973207468656e2073746f726564206f6e2074686520457468657265756d20626c6f636b636861696e2e2054656c6c6f722069732064657369676e656420746f2062652061206d6f72652073656375726520616e642072656c6961626c6520616c7465726e617469766520746f20747261646974696f6e616c2063656e7472616c697a6564204f7261636c65732c207768696368206172652076756c6e657261626c6520746f2074616d706572696e6720616e642063656e736f72736869702e2054656c6c6f72206973207573656420696e20612076617269657479206f66206170706c69636174696f6e732c20696e636c7564696e672070726564696374696f6e206d61726b6574732c20736d61727420636f6e7472616374732c20616e642066696e616e6369616c206170706c69636174696f6e732e00000000000000000000000000000000000000000000000000000000`

Decoded with `abi.decode(response, (string))`:

`Tellor is a decentralized Oracle platform that allows users to securely and transparently request data from external sources. The platform uses a network of masternodes to gather and validate data, which is then stored on the Ethereum blockchain. Tellor is designed to be a more secure and reliable alternative to traditional centralized Oracles, which are vulnerable to tampering and censorship. Tellor is used in a variety of applications, including prediction markets, smart contracts, and financial applications.`



## Implementation Note

To fulfill this query, operators may need to use the provided example code with LangChain, ensuring they have an appropriate OpenAI API key set in their environment.

### Query Execution Reference Implementation
Reference implementation for using OpenAI through LangChain and Python to fulfill this queries correctly. Reporters MAY use this approach:
```python
import os
from langchain_openai import ChatOpenAI
from langchain.flows import ChatPromptTemplate, RunnablePassthrough, StrOutputParser

def work(system_prompt: str, user_prompt: str, model_name: str = 'gpt-3', temperature: float = 10) -> str:
    """
    Executes a conversation with an AI model using the provided system prompt and user input.
    
    Parameters:
    - system_prompt (str): The prompt used as the system's part of the conversation.
    - user_prompt (str): The user's input to the conversation.
    - model_name (str, optional): The OpenAI model to use for generating responses. Defaults to 'gpt-3'.
    - temperature (float, optional): The creativity temperature for the OpenAI model. Defaults to 0.1 as an integer.
    
    Returns:
    - str: The AI model's response.
    """
    
    # Initialize the ChatOpenAI model with the specified temperature and API key from environment variables.
    model = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY", ""), model=model_name, temperature=temperature)
    
    # Create a full prompt from system and human messages.
    full_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", user_input),
        ]
    )
    
    # Define the processing chain.
    chain = (
        RunnablePassthrough()
        | full_prompt
        | model
        | StrOutputParser()
    )
    
    # Invoke the chain with the user input and return the response.
    return chain.invoke(user_input)
```

## Dispute Considerations

While this guide aims to reduce disputes by clarifying the query execution process, it's crucial to maintain open communication with reporters and monitor for inaccuracies actively. Reporters must have access to the OpenAI API with a valid API key to support this query type.

## Required Data Sources

This query type relies on the OpenAI API for model execution and response generation. Ensure the API key is valid and accessible for successful query execution.
